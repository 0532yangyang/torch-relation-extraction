%\section{Training a Sentence Classifier without Alignment \label{sec:uschema}}
\section{Universal Schema as Sentence Classifier \label{sec:uschema}}
%\todo{Reviewer 2: Sec. 3 mainly talks about prediction (matching a text pattern with a KB relation, instead of direct prediction based on entity pairs), but the title is training a sentence classifier}
Similar to many link prediction approaches,~\citep{limin} perform transductive learning, where a model is learned jointly over train and test data. Predictions are made by using the model to identify edges that were unobserved in the test data but likely to be true. The approach is vulnerable to the \emph{cold start} problem in collaborative filtering~\citep{schein2002methods}: it is unclear how to form predictions for unseen entity pairs, without re-factorizing the entire matrix or applying heuristics. 

In response, this paper re-purposes USchema as a means to train a sentence-level relation classifier, like those in Section~\ref{seq:dist}. This allows us to avoid errors from aligning distant supervision to the corpus, but is more deployable for real world applications. It also provides opportunities in Section~\ref{sec:multilingual} to improve multilingual AKBC.

We produce predictions using a very simple approach: (1) scan the corpus and extract a large quantity of triplets $(s,r_{\text{text}},o)$, where $r_{\text{text}}$ is an OpenIE pattern. For each triplet, if the similarity between the embedding of $r_{\text{text}}$ and the embedding of a target relation $r_{\text{schema}}$ is above some threshold, we predict the triplet $(s,r_{\text{schema}},o)$, and its provenance is the input sentence containing $(s,r_{\text{text}},o)$. We refer to this technique as~\textit{pattern scoring}. In our experiments, we use the cosine distance between the vectors.

%% APPENDIX %% In Section~\ref{app:cosine}, we discuss details for how to make this distance well-defined. 


\subsection{Using a Compositional Sentence Encoder to Predict Unseen Text Patterns \label{sec:encoder}}
%\todo{Reviewer 2: Sec. 4 explains how to encode textual patterns with some neural network architectures, and this is clearly used in both training and testing and the title is predictions for unseen text patterns}
The pattern scoring approach is subject to an additional cold start problem: input data may contain patterns unseen in training. This section describes a method for using USchema to train a relation classifier that can take arbitrary context tokens (Section~\ref{sec:openIE}) as input.

Fortunately, the cold start problem for context tokens is more benign than that of entities since we can exploit statistical regularities of text: similar sequences of context tokens should be embedded similarly. Therefore, following \citet{toutanova2015representing}, we  embed raw context tokens compositionally using a deep architecture. Unlike~\citet{limin}, this requires no manual rules to map text to OpenIE patterns and can embed any possible input string. The modified USchema likelihood is:
\begin{equation}
\Prob \left((s,r,o)\right) = \sigma\left( u_{s,o}^\top \text{Encoder}(r) \right).
\end{equation}
Here, if $r$ is raw text, then $\text{Encoder}(r)$ is parameterized by a deep architecture. If $r$ is from the target schema, $\text{Encoder}(r)$ is a produced by a lookup table (as in traditional USchema). Though such an encoder increases the computational cost of test-time prediction over straightforward pattern matching, evaluating a deep architecture can be done in large batches in parallel on a GPU.

Both convolutional networks (CNNs) and recurrent networks (RNNs) are reasonable encoder architectures, and we consider both in our experiments. CNNs have been useful in a variety of NLP applications~\citep{collobert2011natural,KalchbrennerACL2014,kim2014convolutional}. Unlike~\citet{toutanova2015representing}, we also consider RNNs, specifically Long-Short Term Memory Networks (LSTMs)~\citep{lstm}. LSTMs have proven successful in a variety of tasks requiring encoding sentences as vectors~\citep{rnnmt,rnnparse}. In our experiments, LSTMs outperform CNNs.

There are two key differences between our sentence encoder and that of~\citet{toutanova2015representing}.  First, we use the encoder at test time, since we process the context tokens for held-out data. On the other hand,~\citet{toutanova2015representing} adopt the transductive approach where the encoder is only used to help train better representations for the relations in the target schema; it is ignored when forming predictions.  Second, we apply the encoder to the raw text between entities, while~\citet{toutanova2015representing} first perform syntactic dependency parsing on the data and then apply an encoder to the path between the two entities in the parse tree. We avoid parsing, since we seek to perform multilingual AKBC, and many languages lack linguistic resources such as treebanks. Even parsing non-newswire English text, such as tweets, is extremely challenging. %Using the raw data, however, is  challenging since the raw text between two entities may be quite long. On the other hand, training a deep architecture end-to-end to identify the relevant text between entities is more practical for a variety of applications. \todo{pointer to experiments discussing parsed vs. unparsed data}

Prior work has applied deep learning to small-scale relation extraction problems, where functional relationships are detected between common nouns \citep{li2015tree,dos2015classifying}. \citet{xu2015classifying} apply an LSTM to a parse path, while ~\citet{zengdistant} use a CNN on the raw text, with a special temporal pooling operation to separately embed the text around each entity.

\subsection{Modeling Frequent Text Patterns}
\label{sec:non-comp}

Despite the coverage advantages of using a deep sentence encoder, separately embedding each OpenIE pattern, as in~\citet{limin}, has key advantages. In practice, we have found that many high-precision patterns occur quite frequently. For these, there is sufficient data to model them with independent embeddings per pattern, which imposes minimal inductive bias on the relationship between patterns. Furthermore, some discriminative phrases are idiomatic, i.e.. their meaning is not constructed compositionally from their constituents. For these, a sentence encoder may be inappropriate. 
%using a sentence encoder induces shared structure across different patterns' representations. 

Therefore, pattern embeddings and deep token-based encoders have very different strengths and weaknesses. One values specificity, and models the head of the text distribution well, while the other has high coverage and captures the tail. In experimental results, we demonstrate that an ensemble of both models performs substantially better than either in isolation.

%\begin{table}[h!]
%\begin{center}
%\caption{Examples of compositional vs. idiomatic text patterns for relation extraction.\label{tab:patterns2}}
%\begin{tabular}{| p{0.18\textwidth} | p{0.76\textwidth} | }
% \hline
% % who worked face to face with
% % a
% % started out as a
% % Toby Keith hit an emotional note with a performance of
% % , was convicted of all five counts against him , the most serious of them
%\multirow{3}{*}{compositional} & \argOne will be cremated on Friday just outside the city of \argTwo\\ %\cline{2-2}
%& \argOne has pleaded not guilty to charges of \argTwo \\ %\cline{2-2}
%& \argOne was convicted of all five counts against him, including \argTwo \\ %\cline{2-2}
%\hline
%\multirow{3}{*}{idiomatic} & \argOne , aka \argTwo \\ %\cline{2-2}
%& \argOne hit the field in \argTwo \\ %\cline{2-2}
%& \argOne didn't relish the idea of pulling the managerial rug out from under \argTwo \\
%\hline
%\end{tabular}
%\end{center}
%\end{table}


