\section{Appendix}

\subsection{Additional Qualitative Results}

Our model jointly embeds KB relations together with English and Spanish text. We demonstrate that plausible textual patterns are embedded close to the KB relations they express. Table \ref{tab:top-tac-patterns} shows top scoring English and Spanish patterns given sample relations from our TAC KB.

\begin{table}[h]
\begin{center}
\caption{Top scoring patterns for both Spanish and English given query TAC relations. \label{tab:top-tac-patterns}}
%\hspace*{-15pt}
\begin{tabular}{|l|c|p{8.5cm}|}
\hline
 \multirow{3}{*}{per:sibling} &
   \multirow{3}{*}{ES} 	& 	\argOne, seg\'{u}n petici\'{o}n the primeros ministro, \endgraf \hspace{5pt} su hermano gemelo \argTwo  			\\ %\cline{3-3}
   &					&  	\argOne, sea the principal favorito para esto oficina que tambi\'{e}n \endgraf \hspace{5pt} ambiciona su hermano \argTwo 	\\%\cline{3-3}
   &					&  	\argOne, y su hermano gemelo, the primeros ministro \argTwo 	\\
\cline{2-3}
&   \multirow{3}{*}{EN} 	& 	\argOne, for whose brother \argTwo  		\\%\cline{3-3}
   &						&  	\argOne inherited his brother \argTwo 	\\%\cline{3-3}
   &						&  	\argOne on saxophone and brother \argTwo 	\\
\hline\hline
%
 \multirow{3}{*}{org:top\_members\_employees} &
   \multirow{3}{*}{ES} 	& 	\argTwo, presidente y director generales the \argOne  			\\%\cline{3-3}
   &					&  	\argTwo, presidente of the negocios especializada \argOne  	\\%\cline{3-3}
   &					&  	\argTwo (CIA), the director of the entidad, \argOne 	\\
\cline{2-3}
&   \multirow{3}{*}{EN} 	& 	\argTwo, vice president and policy director of the \argOne  		\\%\cline{3-3}
   &						&  	\argTwo, president of the German Soccer \argOne 	\\%\cline{3-3}
   &						&  	\argTwo, president of the quasi-official \argOne 	\\
\hline\hline
%
 \multirow{3}{*}{per:alternate\_names} &
   \multirow{3}{*}{ES} 	& 	\argOne(como tambi\'{e}n son sabido para \argTwo 			\\%\cline{3-3}
   &					&  	\argTwo-cuyos verdaderos nombre sea \argOne 	\\%\cline{3-3}
   &					&  	\argOne  tambi\'{e}n sabido como \argTwo 	\\
\cline{2-3}
&   \multirow{3}{*}{EN} 	& 	\argOne aka \argTwo 		\\%\cline{3-3}
   &						&  	\argOne, who also creates music under the pseudonym \argTwo 	\\%\cline{3-3}
   &						&  	\argOne( of Modern Talking fame ) aka \argTwo  	\\
\hline\hline
%
 \multirow{3}{*}{per:cities\_of\_residence} &
   \multirow{3}{*}{ES} 	& 	\argOne, poblado d\'{o}nde vive \argTwo 			\\%\cline{3-3}
   &					&  	\argOne, una ciudadano naturalizado american y nacido in \argTwo 	\\%\cline{3-3}
   &					&  	\argOne, que vive in \argTwo 	\\
\cline{2-3}
&   \multirow{3}{*}{EN} 	& 	\argOne was born Jan. \# , \#\#\#\# in \argTwo 		\\%\cline{3-3}
   &						&  	\argOne was born on Monday in \argTwo 	\\%\cline{3-3}
   &						&  	\argOne was born at Keighley in \argTwo 	\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Details Concerning Cosine Similarity Computation}
\label{app:cosine}
We measure the similarity between $r_{\text{text}}$ and $r_{\text{schema}}$ by computing the vectors' cosine similarity. However, such a distance is not well-defined, since the model was trained using inner products between entity vectors and relation vectors, not between two relation vectors. The US likelihood is invariant to invertible transformations of the latent coordinate system, since $\sigma\left( u_{s,o}^\top v_r \right) = \sigma\left( (A^\top u_{s,o})^\top A^{-1} v_r \right)$ for any invertible $A$. When taking inner products between two $v$ terms, however, the implicit $A^{-1}$ terms do not cancel out. We found that this issue can be minimized, and high quality predictive accuracy can be achieved, simply by using sufficient $\ell_2$ regularization to avoid implicitly learning an $A$ that substantially stretches the space.

\subsection{Data Pre-processing, Distant Supervision and Extraction Pipeline \label{sec:ds-el}}

We replace tokens occurring less than 5 times in the corpus with UNK and normalize all digits to \# (e.g. Oct-11-1988 becomes Oct-\#\#-\#\#\#\#). 
For each sentence, we then extract all entity pairs and the text between them as surface patterns, ignoring patterns longer than 15 tokens. 
This results in 48 million English `relations'. In Section~\ref{sec:norm}, we describe a technique for normalizing the surface patterns. 
We filter out entity pairs that occurred less than 10 times in the data and extract the largest connected component in this entity co-occurrence graph. 
This is necessary for the baseline US model, as otherwise learning decouples into independent problems per connected component. 
Though the components are connected when using sentence encoders, we use only a single component to facilitate a fair comparison between modeling approaches. 
We add the distant supervision training facts from the RelationFactory system, i.e. 352,236 entity-pair-relation tuples obtained from Freebase and high precision seed patterns.
The final training data contains a set of 3,980,164 (KB and openIE) facts made up of 549,760 unique entity pairs, 1,285,258 unique relations and 62,841 unique tokens.
For entity linking, we make use of the fact that most Freebase entries contain a link to the corresponding Wikipedia page, and we heuristically link all entity mentions from our text corpora to a Freebase entity by the following process: 
First, a set of candidate entities is obtained by following frequent link anchor text statistics. 
We then select that candidate entity for which the cosine similarity between the respective Wikipedia and the sentence context of the mention is highest, and link to that entity if a threshold is exceeded.

We perform the same preprocessing on the Spanish data, resulting in 34 million raw surface patterns between entities. 
We then filter patterns that never occur with an entity pair found in the English data.  This yields 860,502 Spanish patterns. 
Our multilingual model is trained on a combination of these Spanish patterns, the English surface patterns, and the distant supervision data described above. 
We learn word embeddings for 39,912 unique Spanish word types. 
After parameter tying for translation pairs (Section \ref{sec:tie-words}),  there are 33,711 additional Spanish words not tied to English.

We also report results including an alternate names (AN) heuristic, which uses automatically-extracted rules to detect the `alternate name' relation.
For this, frequent Wikipedia link anchor texts are collected for each query entity.
If a high probability anchor text co-occurs with the canonical name of the query in the same document, we return the anchor text as a slot fill.

\subsection{Generation of Cross-Lingual Tied Word Types}
\label{sec:word-tying}
We follow the same procedure for generating translation pairs as \cite{mikolov2013}. First, we select the top 6000 words occurring in the lowercased Europarl dataset for each language and obtain a Google translation. We then filter duplicates and translations resulting in multi-word phrases. We also remove English past participles (ending in -ed) as we found the Google translation interprets these as adjectives (e.g.,  `she read the borrowed book' rather than `she borrowed the book') and much of the relational structure in language we seek to model is captured by verbs. This resulted in 6201 translation pairs that occurred in our text corpus. Though higher quality translation dictionaries would likely improve this technique, our experimental results show that such automatically generated dictionaries perform well.


\subsection{Open IE Pattern Normalization}
\label{sec:norm}
To improve US generalization, our US relations use log-shortened patterns where the middle tokens in patterns longer than five tokens are simplified. For each long pattern we take the first two tokens and last two tokens, and replace all $k$ remaining tokens with the number $\log k$. For example, the pattern {\bf Barack Obama} {\it is married to a person named} {\bf Michelle Obama} would be converted to: {\bf Barack Obama} {\it is married [1] person named} {\bf Michell Obama}. This shortening performs slightly better than whole patterns. LSTM and CNN variants use the entire sequence of tokens.

\subsection {Implementation and Hyperparameters}
\label{sec:details}
All models are implemented in Torch\footnote{\url{http://torch.ch}} and tuned to maximize F1 on the TAC 2012 slot-filling evaluation. We additionally tune the thresholds of our pattern scorer on a per-relation basis to maximize F1 using the 2012 TAC KBP slot filling evaluation as a validation set. All experiments use 50-dimensional relation and entity pair embeddings. Our CNN is implemented as described in \cite{toutanova2015representing}, using width-3 convolutions, followed by tanh and max pool layers. The CNN and LSTM both learned 100-dimensional word embeddings, which were randomly initialized. We found that pre-trained word embeddings did not substantially affect the results. Entity pair embeddings for the baseline US model are randomly initialized. For the models with LSTM and CNN text encoders, entity pair embeddings are initialized using vectors from the baseline US model. This performs better than random initialization. We perform $\ell_2$ gradient clipping to 1 on all models. Universal Schema uses a batch size of 1024 while the CNN and LSTM use 128. All models are optimized using ADAM \citep{kingma2014adam} with $\epsilon=1e-8$, $\beta_1=0.9$, and $\beta_2=0.999$ with a learning rate of .001 for US and .0001 for CNN and LSTM. The CNN and LSTM also use dropout of 0.1 after the embedding layer. All models are trained for a maximum of 15 epochs. We also experimented with bidirectional LSTMs which did not perform better.
