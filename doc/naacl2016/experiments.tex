
%Additionally, as explained in Section \ref{sec:uschema}, models trained to perform link prediction will often suffer from a lack of specificity resulting from learning only to `type check,' i.e. incorrectly predicting that Brad Pitt acted in the movie \emph{The Godfather}. Requiring text provinence makes it much more difficult to make this mistake, since it is highly unlikely that any text will support the above false relation. Embedding entity pairs, rather than embedding single entities then combining their encoded vectors to represent pairs, also helps to avoid the `type checking' problem, and given enough data has been shown to perform better than entity models \citep{toutanova2015representing}. We only experiment on embedding entity pairs as the distinction between entity and entity pairs is orthogonal to this work.


%Many papers evaluate on the popular FB15k dataset for link prediction in knowledge base completion. However, we do not find this interesting as it reduces to little more than type clustering. We are instead interested in learning high quality pattern encoders that, given providence, are able to accurately score the correlation \todo{better word} between patterns and patterns, and patterns and kb relations. In this situation, Brad Pitt would never be inferred to have acted in the Godfather unless there was provinence seen with that entity pair that scored highly with the acted\_in relation. Additionally, this allows us to evaluate unseen entities and address the coldstart problem.

%We choose to use entity pair vectors which, given adequate data, perform better than entity models see \citet{toutanova2015representing} test data with mentions. Although there are many models for kb embedding some using entity vectors and some using entity pair vectors, we do not investigate this distinction as it is orthogonal to our work.



\section{Task and System Description}

We focus on the TAC KBP slot-filling task. Much related work on embedding knowledge bases evaluates on the FB15k dataset \citep{transe,wang2014knowledge,lin2015learning,bishan,toutanova2015representing}. Here, relation extraction is posed as link prediction on a subset of Freebase.  This task does not capture the particular difficulties we address: (1) evaluation on entities and text unseen during training, and (2) zero-annotation learning of a predictor for a low-resource language.

Also, note both~\citet{toutanova2015representing} and~\citet{limin} explore the pros and cons of learning embeddings for entity pairs vs. separate embeddings for each entity. As this is orthogonal to our contributions, we only consider entity pair embeddings, which performed best in both works when given sufficient data.

%, which has received considerable attention in the NLP community since it focuses on a practical AKBC scenario characteristic of various real-world applications.

\subsection{TAC Slot-Filling Benchmark}

The aim of the TAC benchmark is to improve both coverage and quality of relation extraction evaluation compared to just checking the extracted facts against a knowledge base, which can be incomplete and where the provenances are not verified. In the slot-filling task, each system is given a set of paired query entities and relations or `slots' to fill, and the goal is to correctly fill as many slots as possible along with provenance from the corpus. For example, given the query entity/relation pair (\emph{Barack Obama, per:spouse}), the system should return the entity \emph{Michelle Obama} along with sentence(s) whose text expresses that relation. The answers returned by all participating teams, along with a human search (with timeout), are judged manually for correctness, i.e. whether the provenance specified by the system indeed expresses the relation in question.

 %Some slots, such as \emph{per:countries\_of\_residence} can be filled by multiple entities, whereas others, such as \emph{per:country\_of\_birth} can contain only one filler. The query entities are restricted to people (PER) and organizations (ORG) (rather than locations or other noun types, such as religion, which may fill query slots), and the 2013 English evaluation query set is made up of 50 PER entities and 50 ORG entities.

In addition to verifying our models on the 2013 and 2014 English slot-filling task, we evaluate our Spanish models on the 2012 TAC Spanish slot-filling evaluation. Because this TAC track was never officially run, the coverage of facts in the available annotation is very small, resulting in many correct predictions being marked incorrectly as precision errors. In response, we manually annotated all results returned by the models considered in Table~\ref{es-tac-table}. Precision and recall are calculated with respect to the union of the TAC annotation and our new labeling\footnote{Following \citet{surdeanu2012multi} we remove facts about undiscovered entities to correct for recall.}.


% 18 teams participated
% pooled and annotated - not just evaluated against (incomplete) knowledge base, and checked whether fact is actually expressed by text
%  \todo{Ben: something about the popularity of this task. Why  it is a good benchmark. }

\subsection{Retrieval Pipeline \label{sec:pipeline}}
Our retrieval pipeline first generates all valid slot filler candidates for each query entity and slot, based on entities extracted from the corpus using {\sc Factorie} ~\citep{mccallum09:factorie:} to perform tokenization, segmentation, and entity extraction. We perform entity linking by heuristically linking all entity mentions from our text corpora to a Freebase entity using anchor text in Wikipedia. Making use of the fact that most Freebase entries contain a link to the corresponding Wikipedia page, we link all entity mentions from our text corpora to a Freebase entity by the following process:
First, a set of candidate entities is obtained by following frequent link anchor text statistics.
We then select that candidate entity for which the cosine similarity between the respective Wikipedia and the sentence context of the mention is highest, and link to that entity if a threshold is exceeded.

An entity pair qualifies as a candidate prediction if it meets the type criteria for the slot.\footnote{Due to the difficulty of retrieval and entity detection, the maximum recall for predictions is limited. For this reason, \citet{surdeanu2012multi} restrict the evaluation to answer candidates returned by their system and effectively rescaling recall. We do not perform such a re-scaling in our English results in order to compare to other reported results. Our Spanish numbers are rescaled. All scores reflect the `anydoc' (relaxed) scoring to mitigate penalizing effects for systems not included in the evaluation pool.} The TAC 2013 English and Spanish newswire corpora each contain about 1 million newswire documents from 2009--2012. The document retrieval and entity matching components of our relation extraction pipeline are based on RelationFactory~\citep{roth2014relationfactory}, the top-ranked system of the 2013 English slot-filling task. We also use the English distantly supervised training data from this system, which aligns the TAC 2012 corpus to Freebase.
%% APPENDIX %% ; More details on alignment are described in Appendix \ref{sec:ds-el}.

%Our maximum recall is significantly limited by our entity extraction pipeline: $\sim$60\% for English using the TAC `anydoc' (relaxed) scoring. \todo{Ben: juxtapose this with stanford scoring. explain what the SOA numbers are.} 

As discussed in Section~\ref{sec:non-comp}, models using a deep sentence encoder and using a pattern lookup table have complementary strengths and weaknesses. In response, we present results where we ensemble the outputs of the two models by simply taking the union of their individual outputs. Slightly higher results might be obtained through more sophisticated ensembling schemes. % We manually shift the models' thresholds to be more precision-biased, and take the union of the predictions returned by the two models. In contrast,~\citet{toutanova2015representing}, add the confidence scores of the systems and then apply a threshold. We found that this ensembling approach does not adequately account for the qualitative distinction in types of prediction that each technique can make accurately.

\subsection{Data Pre-processing, Distant Supervision and Extraction Pipeline \label{sec:ds-el}}

We replace tokens occurring less than 5 times in the corpus with UNK and normalize all digits to \# (e.g. Oct-11-1988 becomes Oct-\#\#-\#\#\#\#).
For each sentence, we then extract all entity pairs and the text between them as surface patterns, ignoring patterns longer than 20 tokens.
This results in 48 million English `relations'.
We filter out entity pairs that occurred less than 10 times in the data and extract the largest connected component in this entity co-occurrence graph.
This is necessary for the baseline US model, as otherwise learning decouples into independent problems per connected component.
Though the components are connected when using sentence encoders, we use only a single component to facilitate a fair comparison between modeling approaches.
We add the distant supervision training facts from the RelationFactory system, i.e. 352,236 entity-pair-relation tuples obtained from Freebase and high precision seed patterns.
The final training data contains a set of 3,980,164 (KB and openIE) facts made up of 549,760 unique entity pairs, 1,285,258 unique relations and 62,841 unique tokens.

We perform the same preprocessing on the Spanish data, resulting in 34 million raw surface patterns between entities.
We then filter patterns that never occur with an entity pair found in the English data.  This yields 860,502 Spanish patterns.
Our multilingual model is trained on a combination of these Spanish patterns, the English surface patterns, and the distant supervision data described above.
We learn word embeddings for 39,912 unique Spanish word types.
After parameter tying for translation pairs (Section \ref{sec:tie-words}),  there are 33,711 additional Spanish words not tied to English.

We follow the same procedure for generating translation pairs as \cite{mikolov2013}.
First, we select the top 6000 words occurring in the lowercased Europarl dataset for each language and obtain a Google translation.
We then filter duplicates and translations resulting in multi-word phrases.
We also remove English past participles (ending in -ed) as we found the Google translation interprets these as adjectives (e.g.,  `she read the borrowed book' rather than `she borrowed the book') and much of the relational structure in language we seek to model is captured by verbs.
This resulted in 6201 translation pairs that occurred in our text corpus.
Though higher quality translation dictionaries would likely improve this technique, our experimental results show that such automatically generated dictionaries perform well.




\subsection {Model Details \label{sec:models}}
All models are implemented in Torch\footnote{\url{http://torch.ch}}(code publicly available\footnote{\url{https://github.com/patverga/torch-relation-extraction}}).
Models are tuned to maximize F1 on the 2012 TAC KBP slot-filling evaluation.
We additionally tune the thresholds of our pattern scorer on a per-relation basis to maximize F1 using 2012 TAC slot-filling for English and the 2012 Spanish slot-filling development set for Spanish.
As in~\citet{limin}, we train using the BPR loss of~\citet{rendle2009bpr}.
Our CNN is implemented as described in \citet{toutanova2015representing}, using width-3 convolutions, followed by tanh and max pool layers.
The LSTM uses a bi-directional architecture where the forward and backward representations of each hidden state are averaged, followed by max pooling over time.

We performed a small grid search over learning rate {0.0001, 0.005, 0.001}, dropout {0.0, 0.1, 0.25, 0.5}, dimension {50, 100}, $\ell_2$ gradient clipping {1, 10, 50}, and epsilon {1e-8, 1e-6, 1e-4}.
All models are trained for a maximum of 15 epochs.
The CNN and LSTM both use 100d embeddings while USchema uses 50d.
The CNN and LSTM both learned 100-dimensional word embeddings which were randomly initialized.
Using pre-trained embeddings did not substantially affect the results.
Entity pair embeddings for the baseline USchema model are randomly initialized.
For the models with LSTM and CNN text encoders, entity pair embeddings are initialized using vectors from the baseline USchema model.
This performs better than random initialization.
We perform $\ell_2$ gradient clipping to 1 on all models.
Universal Schema uses a batch size of 1024 while the CNN and LSTM use 128.
All models are optimized using ADAM \citep{kingma2014adam} with $\epsilon=1e-8$, $\beta_1=0.9$, and $\beta_2=0.999$ with a learning rate of .001 for USchema and .0001 for CNN and LSTM.
The CNN and LSTM also use dropout of 0.1 after the embedding layer.

We also report results including an alternate names (AN) heuristic, which uses automatically-extracted rules to detect the TAC `alternate name' relation.
To achieve this, we collect frequent Wikipedia link anchor texts for each query entity.
If a high probability anchor text co-occurs with the canonical name of the query in the same document, we return the anchor text as a slot filler.


