export MODEL=UniversalSchema
export NAME=attention-both-lstm

export LOG_ROOT="${TH_RELEX_ROOT}/models/fb15k/$NAME"
export TRAIN_FILE_ROOT="$TH_RELEX_ROOT/data/FB15K-237/fix_padding_no-min//"
export TRAIN_FILE="train-relations.torch"
export FB15K_DIR="$TH_RELEX_ROOT/data/FB15K-237/fix_padding_no-min/eval-data/valid-pooled/"

# export SAVE_MODEL=true
# export LOAD_COL_ENCODER="${TH_RELEX_ROOT}/models/fb15k/lstm-bi-maxpool/tuned/best-model"

export BATCH_SIZE=128
export LEARN_RATE=.001
export L2="0"
export EPSILON="1e-8"
export CLIP_GRADS=100

export MAX_SEQ=500
export MAX_EPOCHS=-1
export EVAL_FREQ=1

export ROW_DIM=100
export COL_DIM=100
export TOKEN_DIM=200
export ROW_ENCODER="lstm"
export COL_ENCODER="lstm"

export TIE_ATTENTION=true
# export TIE_ENCODERS=true

export CRITERION="cross-entropy"
export MODEL_TYPE="neg-sample"
export RELATION_POOL="attention"

export NEG_SAMPLES=2
export PATTERN_DROPOUT=10

