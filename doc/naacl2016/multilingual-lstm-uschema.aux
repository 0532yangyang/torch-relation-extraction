\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{NELL,yago,freebase}
\citation{limin}
\citation{yao2013universal,vector_pra,neelakantan2015compositional,logicmfnaacl15,toutanova2015representing}
\citation{freebase}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:background}{{2}{1}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background }{1}{section.2}}
\citation{yago,freebase}
\citation{nickel2015review}
\citation{rescal,DBLP:journals/corr/Garcia-DuranBUG15,bishan,transe,wang2014knowledge,lin2015learning}
\citation{socherkb}
\citation{bunescu2007learning,distant_supervision,riedel2010modeling,yao2010collective,hoffmann2011knowledge,surdeanu2012multi,min2013distant,zengdistant}
\citation{openie,etzioni2008open,resolver}
\citation{limin}
\citation{limin}
\citation{limin}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Splitting the entities in a multilingual AKBC training set into parts. We only require that entities in the two corpora overlap. Remarkably, we can train a model for the low-resource language even if entities in the low-resource language do not occur in the KB. }}{2}{figure.1}}
\newlabel{tab:multilingual-corpora}{{1}{2}{Splitting the entities in a multilingual AKBC training set into parts. We only require that entities in the two corpora overlap. Remarkably, we can train a model for the low-resource language even if entities in the low-resource language do not occur in the KB}{figure.1}{}}
\newlabel{sec:prediction}{{2.1}{2}{Relation Extraction as Link Prediction}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Relation Extraction as Link Prediction }{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Relation Extraction as Sentence Classification}{2}{subsection.2.2}}
\newlabel{seq:dist}{{2.2}{2}{Relation Extraction as Sentence Classification}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Open-Domain Relation Extraction}{2}{subsection.2.3}}
\newlabel{sec:openIE}{{2.3}{2}{Open-Domain Relation Extraction}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Universal Schema}{2}{subsection.2.4}}
\citation{limin}
\citation{limin}
\citation{rendle2009bpr}
\citation{toutanova2015representing}
\citation{pra,pra_second,vector_pra,neelakantan2015compositional}
\citation{limin}
\citation{schein2002methods}
\citation{toutanova2015representing}
\citation{limin}
\citation{collobert2011natural,KalchbrennerACL2014,kim2014convolutional}
\citation{toutanova2015representing}
\citation{lstm}
\citation{rnnmt,rnnparse}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Examples of sentences expressing relations. Context tokens (italicized) consist of the text occurring between entities (bold) in a sentence. OpenIE patterns are obtained by normalizing the context tokens using hand-coded rules. The top example expresses the per:siblings relation and the bottom two examples both express the per:cities\_of\_residence relation. }}{3}{table.1}}
\newlabel{tab:patterns}{{1}{3}{Examples of sentences expressing relations. Context tokens (italicized) consist of the text occurring between entities (bold) in a sentence. OpenIE patterns are obtained by normalizing the context tokens using hand-coded rules. The top example expresses the per:siblings relation and the bottom two examples both express the per:cities\_of\_residence relation}{table.1}{}}
\newlabel{eq:US-prob}{{1}{3}{Universal Schema}{equation.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Training a Sentence Classifier without Alignment}{3}{section.3}}
\newlabel{sec:uschema}{{3}{3}{Training a Sentence Classifier without Alignment}{section.3}{}}
\newlabel{sec:encoder}{{4}{3}{Predictions for Unseen Text Patterns}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Predictions for Unseen Text Patterns }{3}{section.4}}
\citation{toutanova2015representing}
\citation{toutanova2015representing}
\citation{toutanova2015representing}
\citation{xu2015classifying}
\citation{zengdistant}
\citation{limin}
\citation{zeroshot}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Modeling Frequent Text Patterns}{4}{subsection.4.1}}
\newlabel{sec:non-comp}{{4.1}{4}{Modeling Frequent Text Patterns}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Multilingual Relation Extraction with Zero Annotation}{4}{section.5}}
\newlabel{sec:multilingual}{{5}{4}{Multilingual Relation Extraction with Zero Annotation}{section.5}{}}
\newlabel{sec:tie-words}{{5.1}{4}{Tied Sentence Encoders}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Tied Sentence Encoders }{4}{subsection.5.1}}
\citation{koehn2005europarl}
\citation{Gouws2015,luong2015bilingual,hermann2014multilingual}
\citation{mikolov2013,faruqui2014retrofitting}
\citation{mikolov2013}
\citation{transe,wang2014knowledge,lin2015learning,bishan,toutanova2015representing}
\citation{roth2014relationfactory}
\citation{angeli2014stanford}
\citation{mccallum09:factorie:}
\citation{surdeanu2012multi}
\citation{roth2014relationfactory}
\citation{surdeanu2012multi}
\citation{toutanova2015representing}
\citation{toutanova2015representing}
\citation{limin}
\citation{limin}
\citation{rendle2009bpr}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{5}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Task}{5}{subsection.6.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Precision, recall and F1 of English-only models on the English TAC 2013 slot-filling task. LSTM+USchema ensemble outperforms any single model. }}{6}{table.2}}
\newlabel{en-tac-table}{{2}{6}{Precision, recall and F1 of English-only models on the English TAC 2013 slot-filling task. LSTM+USchema ensemble outperforms any single model}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces F1 scores of multilingual models on the English TAC 2013 slot-filling task. Jointly embedding English and Spanish entity pairs results in higher scores on the English evaluation. }}{6}{table.3}}
\newlabel{en-es-tac-table}{{3}{6}{F1 scores of multilingual models on the English TAC 2013 slot-filling task. Jointly embedding English and Spanish entity pairs results in higher scores on the English evaluation}{table.3}{}}
\newlabel{sec:results}{{6.2}{6}{Results}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Results}{6}{subsection.6.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Zero-Annotation transfer learning F1 scores on 2012 Spanish TAC KBP slot-filling task. Adding a translation dictionary improves all encoder-based models. Ensembling LSTM and USchema models performs the best. }}{6}{table.4}}
\newlabel{es-tac-table}{{4}{6}{Zero-Annotation transfer learning F1 scores on 2012 Spanish TAC KBP slot-filling task. Adding a translation dictionary improves all encoder-based models. Ensembling LSTM and USchema models performs the best}{table.4}{}}
\newlabel{sec:qual-anal}{{6.3}{6}{Qualitative Analysis}{subsection.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Qualitative Analysis }{6}{subsection.6.3}}
\bibdata{sources}
\bibstyle{naaclhlt2016}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Examples of the \emph  {per:children} relation discovered by the LSTM and Universal Schema. Entities are bold and patterns italicized. The LSTM can model a richer set of patterns }}{7}{table.5}}
\newlabel{tab:lstm-us-similar-rels}{{5}{7}{Examples of the \emph {per:children} relation discovered by the LSTM and Universal Schema. Entities are bold and patterns italicized. The LSTM can model a richer set of patterns}{table.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{7}{section.7}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Top English patterns for a Spanish query pattern encoded using the dictionary LSTM: For each Spanish query (English translation in italics), a list of English nearest neighbors. }}{7}{table.6}}
\newlabel{tab:cross-lingual-relations}{{6}{7}{Top English patterns for a Spanish query pattern encoded using the dictionary LSTM: For each Spanish query (English translation in italics), a list of English nearest neighbors}{table.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{7}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Additional Qualitative Results}{7}{subsection.A.1}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Example English query words (not in translation dictionary) in bold with their top nearest neighbors by cosine similarity listed for the dictionary and no ties LSTM variants. Dictionary-tied nearest neighbors are consistently more relevant to the query word than untied. }}{8}{table.7}}
\newlabel{joint-word}{{7}{8}{Example English query words (not in translation dictionary) in bold with their top nearest neighbors by cosine similarity listed for the dictionary and no ties LSTM variants. Dictionary-tied nearest neighbors are consistently more relevant to the query word than untied}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Details Concerning Cosine Similarity Computation}{8}{subsection.A.2}}
\newlabel{app:cosine}{{A.2}{8}{Details Concerning Cosine Similarity Computation}{subsection.A.2}{}}
\newlabel{sec:ds-el}{{A.3}{8}{Data Pre-processing, Distant Supervision and Extraction Pipeline}{subsection.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Data Pre-processing, Distant Supervision and Extraction Pipeline }{8}{subsection.A.3}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Top scoring patterns for both Spanish (top) and English (bottom) given query TAC relations. }}{8}{table.8}}
\newlabel{tab:top-tac-patterns}{{8}{8}{Top scoring patterns for both Spanish (top) and English (bottom) given query TAC relations}{table.8}{}}
\citation{mikolov2013}
\citation{toutanova2015representing}
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Generation of Cross-Lingual Tied Word Types}{9}{subsection.A.4}}
\newlabel{sec:word-tying}{{A.4}{9}{Generation of Cross-Lingual Tied Word Types}{subsection.A.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Open IE Pattern Normalization}{9}{subsection.A.5}}
\newlabel{sec:norm}{{A.5}{9}{Open IE Pattern Normalization}{subsection.A.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Implementation and Hyperparameters}{9}{subsection.A.6}}
\newlabel{sec:details}{{A.6}{9}{Implementation and Hyperparameters}{subsection.A.6}{}}
